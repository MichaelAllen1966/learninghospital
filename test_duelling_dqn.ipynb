{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duelling Double Deep Q Learning - A simple hospital model\n",
    "\n",
    "This is an example of a simple hospital bed model where a Reinforcement learning (RL) agent has to learn how to manage the bed stock:\n",
    "\n",
    "    • Default arrivals = 50/day\n",
    "    • Weekend arrival numbers are 50% average arrival numbers\n",
    "    • Weekday arrival numbers are 120% average arrival numbers\n",
    "    • Distribution of inter-arrival time is inverse exponential\n",
    "    • Average length of stay is 7 days (default)\n",
    "    • Distribution of length of stay is inverse exponential\n",
    "    • The RL agent may request a change in bed numbers once a day (default)\n",
    "    • The allowed bed change requests are -20, -10, 0, 10, 20\n",
    "    • Bed changes take 2 days to occur (default)\n",
    "    • The RL agent receives a reward at each action\n",
    "        ◦ Reward of -1 for each unoccupied bed (default)\n",
    "        ◦ Reward of -1.1 for each patients without a bed(default)\n",
    "    • The simulation is loaded with the average number of patients present\n",
    "    \n",
    "The RL agent must learn to maximise the long term reward (return). The maximum reward = 0, so the agent is learning to minimise the loss for each unoccupied bed or patient without bed.\n",
    "\n",
    "## Reinforcement learning introduction\n",
    "\n",
    "### RL involves:\n",
    "* Trial and error search\n",
    "* Receiving and maximising reward (often delayed)\n",
    "* Linking state -> action -> reward\n",
    "* Must be able to sense something of their environment\n",
    "* Involves uncertainty in sensing and linking action to reward\n",
    "* Learning -> improved choice of actions over time\n",
    "* All models find a way to balance best predicted action vs. exploration\n",
    "\n",
    "### Elements of RL\n",
    "* *Environment*: all observable and unobservable information relevant to us\n",
    "* *Observation*: sensing the environment\n",
    "* *State*: the perceived (or perceivable) environment \n",
    "* *Agent*: senses environment, decides on action, receives and monitors rewards\n",
    "* *Action*: may be discrete (e.g. turn left) or continuous (accelerator pedal)\n",
    "* *Policy* (how to link state to action; often based on probabilities)\n",
    "* *Reward signal*: aim is to accumulate maximum reward over time\n",
    "* *Value function* of a state: prediction of likely/possible long-term reward\n",
    "* *Q*: prediction of likely/possible long-term reward of an *action*\n",
    "* *Advantage*: The difference in Q between actions in a given state (sums to zero for all actions)\n",
    "* *Model* (optional): a simulation of the environment\n",
    "\n",
    "### Types of model\n",
    "\n",
    "* *Model-based*: have model of environment (e.g. a board game)\n",
    "* *Model-free*: used when environment not fully known\n",
    "* *Policy-based*: identify best policy directly\n",
    "* *Value-based*: estimate value of a decision\n",
    "* *Off-policy*: can learn from historic data from other agent\n",
    "* *On-policy*: requires active learning from current decisions\n",
    "\n",
    "\n",
    "## Duelling Deep Q Networks for Reinforcement Learning\n",
    "\n",
    "Q = The expected future rewards discounted over time. This is what we are trying to maximise.\n",
    "\n",
    "The aim is to teach a network to take the current state observations and recommend the action with greatest Q.\n",
    "\n",
    "Duelling is very similar to Double DQN, except that the policy net splits into two. One component reduces to a single value, which will model the state *value*. The other component models the *advantage*, the difference in Q between different actions (the mean value is subtracted from all values, so that the advtantage always sums to zero). These are aggregated to produce Q for each action. \n",
    "\n",
    "<img src=\"./images/duelling_dqn.png\" width=\"500\"/>\n",
    "\n",
    "Q is learned through the Bellman equation, where the Q of any state and action is the immediate reward achieved + the discounted maximum Q value (the best action taken) of next best action, where gamma is the discount rate.\n",
    "\n",
    "$$Q(s,a)=r + \\gamma.maxQ(s',a')$$\n",
    "\n",
    "For a presenation on Q, see https://youtu.be/o22P5rCAAEQ\n",
    "\n",
    "## Key DQN components\n",
    "\n",
    "<img src=\"./images/dqn_components.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "## General method for Q learning:\n",
    "\n",
    "Overall aim is to create a neural network that predicts Q. Improvement comes from improved accuracy in predicting 'current' understood Q, and in revealing more about Q as knowledge is gained (some rewards only discovered after time).\n",
    "\n",
    "<img src=\"./images/dqn_process.png\" width=\"600|\"/>\n",
    "    \n",
    "Target networks are used to stabilise models, and are only updated at intervals. Changes to Q values may lead to changes in closely related states (i.e. states close to the one we are in at the time) and as the network tries to correct for errors it can become unstable and suddenly lose signficiant performance. Target networks (e.g. to assess Q) are updated only infrequently (or gradually), so do not have this instability problem.\n",
    "\n",
    "## Training networks\n",
    "\n",
    "Double DQN contains two networks. This ammendment, from simple DQN, is to decouple training of Q for current state and target Q derived from next state which are closely correlated when comparing input features.\n",
    "\n",
    "The *policy network* is used to select action (action with best predicted Q) when playing the game.\n",
    "\n",
    "When training, the predicted best *action* (best predicted Q) is taken from the *policy network*, but the *policy network* is updated using the predicted Q value of the next state from the *target network* (which is updated from the policy network less frequently). So, when training, the action is selected using Q values from the *policy network*, but the the *policy network* is updated to better predict the Q value of that action from the *target network*. The *policy network* is copied across to the *target network* every *n* steps (e.g. 1000).\n",
    "\n",
    "<img src=\"./images/dqn_training.png\" width=\"700|\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code structure\n",
    "\n",
    "<img src=\"./images/dqn_program_structure.png\" width=\"700|\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           1 Import packages                                  #\n",
    "################################################################################\n",
    "\n",
    "from simpy_envs.env_simple_hospital_bed_1 import HospGym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use a double ended queue (deque) for memory\n",
    "# When memory is full, this will replace the oldest value with the new one\n",
    "from collections import deque\n",
    "\n",
    "# Supress all warnings (e.g. deprecation warnings) for regular use\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           2 Define model parameters                          #\n",
    "################################################################################\n",
    "\n",
    "# Set whether to display on screen (slows model)\n",
    "DISPLAY_ON_SCREEN = False\n",
    "# Discount rate of future rewards\n",
    "GAMMA = 0.95\n",
    "# Learing rate for neural network\n",
    "LEARNING_RATE = 0.001\n",
    "# Maximum number of game steps (state, action, reward, next state) to keep\n",
    "MEMORY_SIZE = 1000000\n",
    "# Sample batch size for policy network update\n",
    "BATCH_SIZE = 3\n",
    "# Number of game steps to play before starting training (all random actions)\n",
    "REPLAY_START_SIZE = 365 * 5\n",
    "# Time step between actions\n",
    "TIME_STEP = 1\n",
    "# Number of steps between policy -> target network update\n",
    "SYNC_TARGET_STEPS = 365\n",
    "# Exploration rate (episolon) is probability of choosign a random action\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "# Reduction in epsilon with each game step\n",
    "EXPLORATION_DECAY = 0.999\n",
    "# Simulation duration\n",
    "SIM_DURATION = 365\n",
    "# Training episodes\n",
    "TRAINING_EPISODES = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                 3 Define DQN (Duelling Deep Q Network) class                 #\n",
    "#                    (Used for both policy and target nets)                    #\n",
    "################################################################################\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    \"\"\"Deep Q Network. Udes for both policy (action) and target (Q) networks.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, neurons_per_layer=48):\n",
    "        \"\"\"Constructor method. Set up neural nets.\"\"\"\n",
    "\n",
    "        # Set starting exploration rate\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        # Set up action space (choice of possible actions)\n",
    "        self.action_space = action_space\n",
    "              \n",
    "        \n",
    "        # First layerswill be common to both Advantage and value\n",
    "        super(DQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(observation_space, neurons_per_layer),\n",
    "            nn.ReLU()            \n",
    "            )\n",
    "        \n",
    "        # Advantage has same number of outputs as the action space\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(neurons_per_layer, neurons_per_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, action_space)\n",
    "            )\n",
    "        \n",
    "        # State value has only one output (one value per state)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(neurons_per_layer, neurons_per_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, 1)\n",
    "            )        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Act either randomly or by redicting action that gives max Q\"\"\"\n",
    "        \n",
    "        # Act randomly if random number < exploration rate\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = random.randrange(self.action_space)\n",
    "            \n",
    "        else:\n",
    "            # Otherwise get predicted Q values of actions\n",
    "            q_values = self.forward(torch.FloatTensor(state))\n",
    "            # Get index of action with best Q\n",
    "            action = np.argmax(q_values.detach().numpy()[0])\n",
    "        \n",
    "        return  action\n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        action_q = value + advantage - advantage.mean()\n",
    "        return action_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                    4 Define policy net training function                     #\n",
    "################################################################################\n",
    "\n",
    "def optimize(policy_net, target_net, memory):\n",
    "    \"\"\"\n",
    "    Update  model by sampling from memory.\n",
    "    Uses policy network to predict best action (best Q).\n",
    "    Uses target network to provide target of Q for the selected next action.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Do not try to train model if memory is less than reqired batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return    \n",
    " \n",
    "    # Reduce exploration rate (exploration rate is stored in policy net)\n",
    "    policy_net.exploration_rate *= EXPLORATION_DECAY\n",
    "    policy_net.exploration_rate = max(EXPLORATION_MIN, \n",
    "                                      policy_net.exploration_rate)\n",
    "    # Sample a random batch from memory\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    for state, action, reward, state_next, terminal in batch:\n",
    "        \n",
    "        state_action_values = policy_net(torch.FloatTensor(state))\n",
    "        \n",
    "        # Get target Q for policy net update\n",
    "       \n",
    "        if not terminal:\n",
    "            # For non-terminal actions get Q from policy net\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach next state values from gradients to prevent updates\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Get next state action with best Q from the policy net (double DQN)\n",
    "            policy_next_state_values = policy_net(torch.FloatTensor(state_next))\n",
    "            policy_next_state_values = policy_next_state_values.detach()\n",
    "            best_action = np.argmax(policy_next_state_values[0].numpy())\n",
    "            # Get target net next state\n",
    "            next_state_action_values = target_net(torch.FloatTensor(state_next))\n",
    "            # Use detach again to prevent target net gradients being updated\n",
    "            next_state_action_values = next_state_action_values.detach()\n",
    "            best_next_q = next_state_action_values[0][best_action].numpy()\n",
    "            updated_q = reward + (GAMMA * best_next_q)      \n",
    "            expected_state_action_values[0][action] = updated_q\n",
    "        else:\n",
    "            # For termal actions Q = reward (-1)\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach values from gradients to prevent gradient update\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Set Q for all actions to reward (-1)\n",
    "            expected_state_action_values[0] = reward\n",
    " \n",
    "        # Set network to training mode\n",
    "        policy_net.train()\n",
    "        # Reset net gradients\n",
    "        policy_net.optimizer.zero_grad()  \n",
    "        # calculate loss\n",
    "        loss_v = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "        # Backpropogate loss\n",
    "        loss_v.backward()\n",
    "        # Update network gradients\n",
    "        policy_net.optimizer.step()  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                            5 Define memory class                             #\n",
    "################################################################################\n",
    "\n",
    "class Memory():\n",
    "    \"\"\"\n",
    "    Replay memory used to train model.\n",
    "    Limited length memory (using deque, double ended queue from collections).\n",
    "      - When memory full deque replaces oldest data with newest.\n",
    "    Holds, state, action, reward, next state, and episode done.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor method to initialise replay memory\"\"\"\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"state/action/reward/next_state/done\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                       6  Define results plotting function                    #\n",
    "################################################################################\n",
    "\n",
    "def plot_results(run, exploration, score, run_details):\n",
    "    \"\"\"Plot and report results at end of run\"\"\"\n",
    "    \n",
    "    # Get beds and patients from run_detals DataFrame\n",
    "    beds = run_details['beds']\n",
    "    patients = run_details['patients']    \n",
    "    \n",
    "    # Set up chart (ax1 and ax2 share x-axis to combine two plots on one graph)\n",
    "    fig = plt.figure(figsize=(9,5))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot results\n",
    "    average_rewards = np.array(score)/SIM_DURATION\n",
    "    ax1.plot(run, exploration, label='exploration', color='g')\n",
    "    ax2.plot(run, average_rewards, label='average reward', color='r')\n",
    "    \n",
    "    # Set axes\n",
    "    ax1.set_xlabel('run')\n",
    "    ax1.set_ylabel('exploration', color='g')\n",
    "    ax2.set_ylabel('average reward', color='r')\n",
    "    \n",
    "    # Show last run tracker of beds and patients\n",
    "\n",
    "    ax3 = fig.add_subplot(122)\n",
    "    day = np.arange(len(beds))*TIME_STEP\n",
    "    ax3.plot(day, beds, label='beds', color='g')\n",
    "    ax3.plot(day, patients, label='patients', color='r')\n",
    "    \n",
    "    # Set axes\n",
    "    ax3.set_xlabel('day')\n",
    "    ax3.set_ylabel('beds/patients')\n",
    "    ax3.set_ylim(0)\n",
    "    ax3.legend()\n",
    "    ax3.grid()\n",
    "    # Show\n",
    "    \n",
    "    plt.tight_layout(pad=2)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate summary results\n",
    "    results = pd.Series()\n",
    "    beds = np.array(beds)\n",
    "    patients = np.array(patients)\n",
    "    results['days under capacity'] = np.sum(patients > beds)\n",
    "    results['days over capacity'] = np.sum(beds > patients)\n",
    "    results['average patients'] = np.round(np.mean(patients), 0)\n",
    "    results['average beds'] = np.round(np.mean(beds), 0)\n",
    "    results['% occupancy'] = np.round((patients.sum() / beds.sum() * 100), 1)\n",
    "    print (results);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                                 7 Main program                               #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def hosp_bed_management():\n",
    "    \"\"\"Main program loop\"\"\"\n",
    "    \n",
    "    ############################################################################\n",
    "    #                          8 Set up environment                            #\n",
    "    ############################################################################\n",
    "        \n",
    "    # Set up game environemnt\n",
    "    sim = HospGym(sim_duration=SIM_DURATION, time_step=TIME_STEP)\n",
    "\n",
    "    # Get number of observations returned for state\n",
    "    observation_space = sim.observation_size\n",
    "    \n",
    "    # Get number of actions possible\n",
    "    action_space = sim.action_size\n",
    "    \n",
    "    ############################################################################\n",
    "    #                    9 Set up policy and target nets                       #\n",
    "    ############################################################################\n",
    "    \n",
    "    # Set up policy and target neural nets\n",
    "    policy_net = DQN(observation_space, action_space)\n",
    "    target_net = DQN(observation_space, action_space)\n",
    "    \n",
    "    # Set loss function and optimizer\n",
    "    policy_net.optimizer = optim.Adam(\n",
    "            params=policy_net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Copy weights from policy_net to target\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # Set target net to eval rather than training mode\n",
    "    # We do not train target net - ot is copied from policy net at intervals\n",
    "    target_net.eval()\n",
    "    \n",
    "    ############################################################################\n",
    "    #                            10 Set up memory                              #\n",
    "    ############################################################################\n",
    "        \n",
    "    # Set up memomry\n",
    "    memory = Memory()\n",
    "    \n",
    "    ############################################################################\n",
    "    #                     11 Set up + start training loop                      #\n",
    "    ############################################################################\n",
    "    \n",
    "    # Set up run counter and learning loop    \n",
    "    run = 0\n",
    "    all_steps = 0\n",
    "    continue_learning = True\n",
    "    \n",
    "    # Set up list for results\n",
    "    results_run = []\n",
    "    results_exploration = []\n",
    "    results_score = []\n",
    "    \n",
    "    # Continue repeating games (episodes) until target complete\n",
    "    while continue_learning:\n",
    "        \n",
    "        ########################################################################\n",
    "        #                           12 Play episode                            #\n",
    "        ########################################################################\n",
    "        \n",
    "        # Increment run (episode) counter\n",
    "        run += 1\n",
    "        \n",
    "        ########################################################################\n",
    "        #                             13 Reset game                            #\n",
    "        ########################################################################\n",
    "        \n",
    "        # Reset game environment and get first state observations\n",
    "        state = sim.reset()\n",
    "        \n",
    "        # Trackers for state\n",
    "        weekday = []\n",
    "        beds = []\n",
    "        patients = []\n",
    "        spare_beds = []\n",
    "        pending_change = []\n",
    "        rewards = [] \n",
    "        \n",
    "        # Reset total reward\n",
    "        total_reward = 0    \n",
    "   \n",
    "        # Reshape state into 2D array with state obsverations as first 'row'\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        \n",
    "        # Continue loop until episode complete\n",
    "        while True:\n",
    "            \n",
    "        ########################################################################\n",
    "        #                       14 Game episode loop                           #\n",
    "        ########################################################################\n",
    "\n",
    "            ####################################################################\n",
    "            #                       15 Get action                              #\n",
    "            ####################################################################\n",
    "            \n",
    "            # Get action to take (use evalulation mode)\n",
    "            policy_net.eval()\n",
    "            action = policy_net.act(state)\n",
    "            \n",
    "            ####################################################################\n",
    "            #                 16 Play action (get S', R, T)                    #\n",
    "            ####################################################################\n",
    "            \n",
    "            # Act\n",
    "            state_next, reward, terminal, info = sim.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update trackers\n",
    "            weekday.append(state_next[0])\n",
    "            beds.append(state_next[1])\n",
    "            patients.append(state_next[2])\n",
    "            spare_beds.append(state_next[3])\n",
    "            pending_change.append(state_next[4])\n",
    "            rewards.append(reward)\n",
    "                                                          \n",
    "            # Reshape state into 2D array with state obsverations as first 'row'\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            \n",
    "            # Update display if needed\n",
    "            if DISPLAY_ON_SCREEN:\n",
    "                sim.render()\n",
    "            \n",
    "            ####################################################################\n",
    "            #                  17 Add S/A/R/S/T to memory                      #\n",
    "            ####################################################################\n",
    "            \n",
    "            # Record state, action, reward, new state & terminal\n",
    "            memory.remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            # Update state\n",
    "            state = state_next\n",
    "            \n",
    "            ####################################################################\n",
    "            #                  18 Check for end of episode                     #\n",
    "            ####################################################################\n",
    "            \n",
    "            # Actions to take if end of game episode\n",
    "            if terminal:\n",
    "                # Get exploration rate\n",
    "                exploration = policy_net.exploration_rate\n",
    "                # Clear print row content\n",
    "                clear_row = '\\r' + ' '*79 + '\\r'\n",
    "                print (clear_row, end ='')\n",
    "                print (f'Run: {run}, ', end='')\n",
    "                print (f'Exploration: {exploration: .3f}, ', end='')\n",
    "                average_reward = total_reward/SIM_DURATION\n",
    "                print (f'Average reward: {average_reward:4.1f}', end='')\n",
    "                \n",
    "                # Add to results lists\n",
    "                results_run.append(run)\n",
    "                results_exploration.append(exploration)\n",
    "                results_score.append(total_reward)\n",
    "                \n",
    "                ################################################################\n",
    "                #             18b Check for end of learning                    #\n",
    "                ################################################################\n",
    "                \n",
    "                if run == TRAINING_EPISODES:\n",
    "                    continue_learning = False\n",
    "                \n",
    "                # End episode loop\n",
    "                break\n",
    "            \n",
    "            \n",
    "            ####################################################################\n",
    "            #                        19 Update policy net                      #\n",
    "            ####################################################################\n",
    "            \n",
    "            # Avoid training model if memory is not of sufficient length\n",
    "            if len(memory.memory) > REPLAY_START_SIZE:\n",
    "        \n",
    "                # Update policy net\n",
    "                optimize(policy_net, target_net, memory.memory)\n",
    "\n",
    "                ################################################################\n",
    "                #             20 Update target net periodically                #\n",
    "                ################################################################\n",
    "                \n",
    "                # Use load_state_dict method to copy weights from policy net\n",
    "                if all_steps % SYNC_TARGET_STEPS == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "    ############################################################################\n",
    "    #                      21 Learning complete - plot results                 #\n",
    "    ############################################################################\n",
    "\n",
    "    # Add last run to DataFrame. summarise, and return\n",
    "    run_details = pd.DataFrame()\n",
    "    run_details['weekday'] = weekday \n",
    "    run_details['beds'] = beds\n",
    "    run_details['patients'] = patients\n",
    "    run_details['spare_beds'] = spare_beds\n",
    "    run_details['pending_change'] = pending_change\n",
    "    run_details['reward'] = rewards    \n",
    "        \n",
    "    # Target reached. Plot results\n",
    "    plot_results(\n",
    "        results_run, results_exploration, results_score, run_details)\n",
    "    \n",
    "    return run_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model and return last run results by day\n",
    "last_run = hosp_bed_management()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
