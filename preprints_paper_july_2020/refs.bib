
@article{brockman_openai_2016,
	title = {OpenAI Gym},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2020-07-09},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/ZGBS4UWZ/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/JEBPBTPR/1606.html:text/html}
}

@article{li_deep_2018,
	title = {Deep Reinforcement Learning: An Overview},
	shorttitle = {Deep {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1701.07274},
	abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
	urldate = {2020-07-09},
	journal = {arXiv:1701.07274 [cs]},
	author = {Li, Yuxi},
	month = nov,
	year = {2018},
	note = {arXiv: 1701.07274},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/9TTTDEYZ/Li - 2018 - Deep Reinforcement Learning An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/KHM4HZ3R/1701.html:text/html}
}

@inproceedings{monks_using_2017,
	title = {Using simulation to help hospitals reduce emergency department waiting times: {Examples} and impact},
	shorttitle = {Using simulation to help hospitals reduce emergency department waiting times},
	doi = {10.1109/WSC.2017.8248000},
	abstract = {In recent years, all acute hospitals in the UK have experienced unprecedented emergency department waiting times and hospital bed pressures. The consequences are overcrowded emergency departments, ambulance shortages, cancelled elective operations, low staff morale and financial penalties. To deal with the increasing numbers of patient admissions and delayed discharges hospitals must turn now to modelling and simulation to help increase their flexibility and ability to deal with demand variation. Hospitals face several issues that reduce their flexibility including the need for extreme value-for-money and specialization of care. This talk presents three ED case studies undertaken by an analytics team in the UK. The paper considers the impact of the work and challenges arising from their experiences of simulation modelling in acute hospitals. Final thoughts consider the future of ED simulation.},
	booktitle = {2017 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Monks, Thomas and Meskarian, Rudabeh},
	month = dec,
	year = {2017},
	note = {ISSN: 1558-4305},
	keywords = {Hospitals, health care, ambulance shortages, cancelled elective operations, Clocks, discharges hospitals, ED simulation, emergency department, emergency services, financial penalties, hospital bed pressures, hospitals, low staff morale, medical administrative data processing, overcrowded emergency departments, Partial discharges, patient admissions, patient care, Servers, simulation modelling, UK, value-for-money, waiting time reduction},
	pages = {2752--2763},
	file = {IEEE Xplore Abstract Record:/home/michael/Dropbox/04_Zotero_database/storage/93U4FK6I/8248000.html:text/html}
}

@article{penn_towards_2020,
	title = {Towards generic modelling of hospital wards: Reuse and redevelopment of simple models},
	volume = {14},
	issn = {1747-7778},
	shorttitle = {Towards generic modelling of hospital wards},
	url = {https://doi.org/10.1080/17477778.2019.1664264},
	doi = {10.1080/17477778.2019.1664264},
	abstract = {Generic simulation models are designed to enable model reuse. We argue that there are two weaknesses within the generic simulation modelling literature. Firstly, that generic models sacrifice the relative simplicity of a bespoke simulation model for flexibility. Secondly, that generic models are published in conceptual form only. If researchers cannot access computer implementation of models, then there is little incentive or benefit to recode one over coding a simpler bespoke simulation model. We introduce an incremental approach to generic modelling in discrete-event simulation. We develop an archetype setting-specific generic model of a hospital ward. The archetype model is first developed and applied in a rehabilitation ward setting. Then a second team applies the model in a specialised intensive care setting. We report the successes, obstacles and redevelopment needed for reuse of the generic model along with how the results of these studies were used to inform healthcare delivery.},
	number = {2},
	urldate = {2020-07-09},
	journal = {Journal of Simulation},
	author = {Penn, M. L. and Monks, T. and Kazmierska, A. A. and Alkoheji, M. R. A. R.},
	month = apr,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17477778.2019.1664264},
	keywords = {discrete event simulation, Generic modelling, model redevelopment, model reuse},
	pages = {107--118},
	file = {Full Text PDF:/home/michael/Dropbox/04_Zotero_database/storage/6INE74E2/Penn et al. - 2020 - Towards generic modelling of hospital wards Reuse.pdf:application/pdf;Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/3KS32MAJ/17477778.2019.html:text/html}
}

@article{monks_modelling_2016,
	title = {A modelling tool for capacity planning in acute and community stroke services},
	volume = {16},
	issn = {1472-6963},
	url = {https://doi.org/10.1186/s12913-016-1789-4},
	doi = {10.1186/s12913-016-1789-4},
	abstract = {Mathematical capacity planning methods that can take account of variations in patient complexity, admission rates and delayed discharges have long been available, but their implementation in complex pathways such as stroke care remains limited. Instead simple average based estimates are commonplace. These methods often substantially underestimate capacity requirements.},
	language = {en},
	number = {1},
	urldate = {2020-07-09},
	journal = {BMC Health Services Research},
	author = {Monks, Thomas and Worthington, David and Allen, Michael and Pitt, Martin and Stein, Ken and James, Martin A.},
	month = sep,
	year = {2016},
	pages = {530},
	file = {Springer Full Text PDF:/home/michael/Dropbox/04_Zotero_database/storage/AJWVQ2HH/Monks et al. - 2016 - A modelling tool for capacity planning in acute an.pdf:application/pdf}
}

@article{team_simpy_simpy_2020,
	title = {SimPy. Discrete event simulation for Python.},
	journal = {https://simpy.readthedocs.io/en/latest/},
	url = {https://simpy.readthedocs.io/en/latest/},
	urldate = {2020-07-09},
	author = {SimPy,},
	year = {2020}
}


@article{allen_can_2019,
	title = {Can clinical audits be enhanced by pathway simulation and machine learning? {An} example from the acute stroke pathway},
	volume = {9},
	copyright = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. Published by BMJ.. This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Can clinical audits be enhanced by pathway simulation and machine learning?},
	url = {https://bmjopen.bmj.com/content/9/9/e028296},
	doi = {10.1136/bmjopen-2018-028296},
	abstract = {Objective To evaluate the application of clinical pathway simulation in machine learning, using clinical audit data, in order to identify key drivers for improving use and speed of thrombolysis at individual hospitals.
Design Computer simulation modelling and machine learning.
Setting Seven acute stroke units.
Participants Anonymised clinical audit data for 7864 patients.
Results Three factors were pivotal in governing thrombolysis use: (1) the proportion of patients with a known stroke onset time (range 44\%–73\%), (2) pathway speed (for patients arriving within 4 hours of onset: per-hospital median arrival-to-scan ranged from 11 to 56 min; median scan-to-thrombolysis ranged from 21 to 44 min) and (3) predisposition to use thrombolysis (thrombolysis use ranged from 31\% to 52\% for patients with stroke scanned with 30 min left to administer thrombolysis). A pathway simulation model could predict the potential benefit of improving individual stages of the clinical pathway speed, whereas a machine learning model could predict the benefit of ‘exporting’ clinical decision making from one hospital to another, while allowing for differences in patient population between hospitals. By applying pathway simulation and machine learning together, we found a realistic ceiling of 15\%–25\% use of thrombolysis across different hospitals and, in the seven hospitals studied, a realistic opportunity to double the number of patients with no significant disability that may be attributed to thrombolysis.
Conclusions National clinical audit may be enhanced by a combination of pathway simulation and machine learning, which best allows for an understanding of key levers for improvement in hyperacute stroke pathways, allowing for differences between local patient populations. These models, based on standard clinical audit data, may be applied at scale while providing results at individual hospital level. The models facilitate understanding of variation and levers for improvement in stroke pathways, and help set realistic targets tailored to local populations.},
	language = {en},
	number = {9},
	urldate = {2019-09-17},
	journal = {BMJ Open},
	author = {Allen, Michael and Pearn, Kerry and Monks, Thomas and Bray, Benjamin D. and Everson, Richard and Salmon, Andrew and James, Martin and Stein, Ken},
	month = sep,
	year = {2019},
	keywords = {Stroke, thrombolysis, alteplase, simulation, machine learning, health services research},
	pages = {e028296},
	file = {Full Text PDF:/home/michael/Dropbox/04_Zotero_database/storage/MKHM8DL6/Allen et al. - 2019 - Can clinical audits be enhanced by pathway simulat.pdf:application/pdf;Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/JZ38HKVW/e028296.html:text/html}
}


@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2020-03-10},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/T9RB4UH6/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/FBWJ7T6R/1509.html:text/html}
}

@article{fortunato_noisy_2019,
	title = {Noisy {Networks} for {Exploration}},
	url = {http://arxiv.org/abs/1706.10295},
	abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \${\textbackslash}epsilon\$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
	urldate = {2020-03-12},
	journal = {arXiv:1706.10295 [cs, stat]},
	author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
	month = jul,
	year = {2019},
	note = {arXiv: 1706.10295},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/I8X39ZGQ/Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/J4QUFPUR/1706.html:text/html}
}

@article{wang_dueling_2016,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2020-03-14},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/PIYBPMD3/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/C26DDBV9/1511.html:text/html}
}

@article{osband_deep_2016,
	title = {Deep {Exploration} via {Bootstrapped} {DQN}},
	url = {http://arxiv.org/abs/1602.04621},
	abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
	urldate = {2020-06-10},
	journal = {arXiv:1602.04621 [cs, stat]},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	month = jul,
	year = {2016},
	note = {arXiv: 1602.04621},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/HUEJWYEN/Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/Y66NEEY2/1602.html:text/html}
}

@article{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2020-07-21},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Dropbox/04_Zotero_database/storage/WJ7BDCZH/Schaul et al. - 2016 - Prioritized Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Dropbox/04_Zotero_database/storage/T4IEPXGF/1511.html:text/html}
}
