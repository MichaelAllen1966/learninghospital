{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing CartPole\n",
    "\n",
    "Cartpole is a classic control problem from OpenAI Gym, a set of reinforcement learning environments for testing and developing RL agents.\n",
    "\n",
    "https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Gym methods\n",
    "\n",
    "* Make method, e.g. `env = gym.make(\"CartPole-v1\")`\n",
    "\n",
    "* Reset (returns first state observations), e.g. `env.reset()`\n",
    "\n",
    "* Step (passes action to environemtn and returns observations, reward, done*, and any extra info), e.g. `obs, reward, done, info = env.step(action)`\n",
    "\n",
    "* Close  (closes environment at end), e.g. `env.close()`\n",
    "\n",
    "*With the step method, the environment returns whether it has reached a terminal state, that is the end of an epsiode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set whether enviornment will be rendered\n",
    "RENDER = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random choice\n",
    "\n",
    "Our first baseline is random action of pushing the cart left or right.\n",
    "\n",
    "Note: The CartPole visualisation of this demo may not work on remote servers. If this does not work set `RENDER = False` in cell above to run rest of Notebook with visualisation (re-run the cell after changing the setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(obs):\n",
    "    \"\"\"\n",
    "    Random choice. \n",
    "    `obs` is passed to function to make use consistent with other methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    return random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 16.4\n",
      "Stdev: 4.6\n",
      "Minumum: 11\n",
      "Maximum: 26\n"
     ]
    }
   ],
   "source": [
    "# Set up environemnt\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "totals = []\n",
    "for episode in range(10):\n",
    "    episode_reward = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        action = random_choice(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        # Pole has fallen over if done is True\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_reward)\n",
    "    env.close()\n",
    "\n",
    "print (\"Average: {0:.1f}\".format(np.mean(totals)))    \n",
    "print (\"Stdev: {0:.1f}\".format(np.std(totals)))\n",
    "print (\"Minumum: {0:.0f}\".format(np.min(totals)))\n",
    "print (\"Maximum: {0:.0f}\".format(np.max(totals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple policy\n",
    "\n",
    "Here we use a simple policy that accelerates left when the pole is leaning to the right, and accelerates right when the pole is leaning to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Simple policy that accelerates left when the pole is leaning to the right,\n",
    "    and accelerates right when the pole is leaning to the left\n",
    "\n",
    "    Cartpole observations:\n",
    "        X position (0 = centre)\n",
    "        velocity (+ve = right)\n",
    "        angle (0 = upright)\n",
    "        angular velocity (+ve = clockwise)\n",
    "    \"\"\"\n",
    "    \n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 44.2\n",
      "Stdev: 10.7\n",
      "Minumum: 25\n",
      "Maximum: 63\n"
     ]
    }
   ],
   "source": [
    "# Set up environemnt\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "totals = []\n",
    "for episode in range(10):\n",
    "    episode_reward = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        if RENDER:\n",
    "                env.render()\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        # Pole has fallen over if done is True\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_reward)\n",
    "    env.close()\n",
    "\n",
    "print (\"Average: {0:.1f}\".format(np.mean(totals)))    \n",
    "print (\"Stdev: {0:.1f}\".format(np.std(totals)))\n",
    "print (\"Minumum: {0:.0f}\".format(np.min(totals)))\n",
    "print (\"Maximum: {0:.0f}\".format(np.max(totals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next notebook will use a Deep Q Network (Double DQN) to see if we can improve on the simple policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
